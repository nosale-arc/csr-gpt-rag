{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPbykcfj9Bai0KSvOMFRgBR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nosale-arc/csr-gpt-rag/blob/main/CSR_RAG_POC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XfyrJTXV4wY9"
      },
      "outputs": [],
      "source": [
        "! pip3 install --upgrade --user langchain_community langchain-openai pypdf chromadb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Restart kernel after installs so that your environment can access the new packages\n",
        "import IPython\n",
        "import time\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ],
      "metadata": {
        "id": "4ESK_iGI6_jb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ],
      "metadata": {
        "id": "rL7D8HHDu2cQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# First load the PDF document into memory"
      ],
      "metadata": {
        "id": "hRtN41giu4MX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
        "DOC_PATH = \"/content/Nelson_Biology_unit_30d.pdf\"\n",
        "CHROMA_PATH = \"nelson_db\"\n",
        "\n",
        "# load your pdf doc\n",
        "loader = PyPDFLoader(DOC_PATH)\n",
        "pages = loader.load()"
      ],
      "metadata": {
        "id": "7msbmA4E5gLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Split the document into chunks"
      ],
      "metadata": {
        "id": "a5ktkvsiu-1w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# split the doc into smaller chunks i.e. chunk_size=500\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "chunks = text_splitter.split_documents(pages)"
      ],
      "metadata": {
        "id": "7se-xxlB7RPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load the openAI key"
      ],
      "metadata": {
        "id": "rp7QP5m9vDo_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "OPENAI_API_KEY = userdata.get('openai_key')"
      ],
      "metadata": {
        "id": "U-3B9e2r8Ael"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Set up the vector database and define how to answer the question"
      ],
      "metadata": {
        "id": "R6CNqhSnvG5h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# get OpenAI Embedding model\n",
        "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
        "\n",
        "# embed the chunks as vectors and load them into the database\n",
        "db_chroma = Chroma.from_documents(chunks, embeddings, persist_directory=CHROMA_PATH)\n",
        "\n",
        "# Change persona to alter the style of the output\n",
        "# some possibilities: \"a pirate\", \"Shakespeare\", \"a 5 year old\"\n",
        "# \"Steve Jobs\" or \"an Engineer\" tend to produce the most accurate results\n",
        "def answer_question(query, persona=\"Steve Jobs\"):\n",
        "\n",
        "  # retrieve context - top 10 most relevant (closest) chunks to the query vector\n",
        "  # (by default Langchain is using cosine distance metric)\n",
        "  docs_chroma = db_chroma.similarity_search_with_score(query, k=10)\n",
        "\n",
        "  # generate an answer based on given user query and retrieved context information\n",
        "  context_text = \"\\n\\n\".join([doc.page_content for doc, _score in docs_chroma])\n",
        "\n",
        "  # you can use a prompt template\n",
        "  PROMPT_TEMPLATE = \"\"\"\n",
        "  Answer the question based only on the following context:\n",
        "  {context}\n",
        "  Answer the question based on the above context: {question}.\n",
        "  Provide a detailed answer.\n",
        "  Don’t justify your answers.\n",
        "  Don’t give information not mentioned in the CONTEXT INFORMATION.\n",
        "  Do not say \"according to the context\" or \"mentioned in the context\" or similar.\n",
        "  Answer this question as if you were {persona}\n",
        "  \"\"\"\n",
        "\n",
        "  # load retrieved context and user query in the prompt template\n",
        "  prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
        "  prompt = prompt_template.format(context=context_text, question=query, persona = persona)\n",
        "\n",
        "  # call LLM model to generate the answer based on the given context and query\n",
        "  model = ChatOpenAI(openai_api_key=OPENAI_API_KEY)\n",
        "  response_text = model.invoke(prompt)\n",
        "\n",
        "  return response_text"
      ],
      "metadata": {
        "id": "vX3fKvFM7WzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(answer_question(\"What is the definition of a trait?\"))"
      ],
      "metadata": {
        "id": "0eP3t-PaCYtO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dWSnYUdhDGXk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}